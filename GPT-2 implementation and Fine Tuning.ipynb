{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea91338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\zohaib\\miniconda3\\envs\\createdata\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zohaib\\miniconda3\\envs\\createdata\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zohaib\\miniconda3\\envs\\createdata\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ---------------------------------------- 0/3 [pytz]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   ------------- -------------------------- 1/3 [tzdata]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   -------------------------- ------------- 2/3 [pandas]\n",
      "   ---------------------------------------- 3/3 [pandas]\n",
      "\n",
      "Successfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ea23373-fb10-4a6f-a24d-926f3578ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c866a1a2-b9d3-485d-9f47-6058e580a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516e6f61-2e4e-4e9f-abfe-49157edf5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "    \n",
    "    with urllib.request.urlopen(url) as response:   \n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:   \n",
    "        zip_ref.extractall(extracted_path)\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)              \n",
    "    print(f\"File downloaded and saved as {data_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010ca850-47a8-4f40-bec8-5e2fe5f13813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as sms_spam_collection\\SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "source": [
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b9ba302-7c3a-43e6-86f4-e192af1b1db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('sms_spam_collection/SMSSpamCollection.tsv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a67726af-0518-4091-8402-d08a69526c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0088576-ed19-4e5a-9a79-aea47edc559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdceebde-3ac5-46df-9ade-eff548bc3934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]    \n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)                              \n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])                             \n",
    "    return balanced_df\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4317ba5-dabf-4cae-ae70-cbdd159f2d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd70af81-21e8-4362-b227-d899f4c20a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)              \n",
    "    train_end = int(len(df) * train_frac)         \n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb458491-9a9c-4716-9783-c9ea686e14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2521f519-299b-4a7b-a2fe-c0f06368d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "207882f6-3512-4c6d-9ee3-58f50650df65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0bc4b02-e0ef-4917-aedb-04b428169ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1494"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f5aecf6-c456-4e74-80bd-2195bcc579a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaa45043-7d3f-498a-bff7-cf5436b0ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e96b6f6-b179-4b95-92aa-720dc3dfe4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_length = None, pad_id = 50256):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_id = pad_id\n",
    "        self.data = pd.read_csv(path)\n",
    "        #self.data = self.data.iloc[0]\n",
    "        if max_length is None:\n",
    "            self.max_length = self.get_max_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "        self.data['encoded_text'] = self.data['Text'].apply(self.encode)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data.iloc[idx]['encoded_text'], dtype = torch.long), torch.tensor(self.data.iloc[idx]['Label'], dtype = torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def encode(self, text):\n",
    "        length = len(self.tokenizer.encode(text))\n",
    "        array = [self.pad_id] * (self.max_length - length)\n",
    "        encoding = self.tokenizer.encode(text)\n",
    "        encoding.extend(array)\n",
    "        return encoding\n",
    "    \n",
    "    def get_max_length(self):\n",
    "        max_idx = df['Text'].str.len().idxmax()\n",
    "        return len(self.tokenizer.encode(df.iloc[max_idx]['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcd287ae-022c-42bf-9497-cb05c458a817",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "731bf1ba-9273-4dbb-b0a4-07545f25ebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset('train.csv', tokenizer = tokenizer)\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88f19c41-dd38-4605-b803-28aa63c0da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    \"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1023490e-5d4f-4f13-a0f5-695d5b2fc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SpamDataset(\n",
    "    \"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dffdcc5-b8cd-4409-a20a-b74590a4cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = True, drop_last = False)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = True, drop_last = False)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, num_workers = num_workers, shuffle = True, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cd4ae30-4312-4542-8615-c31aadd183df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([5, 216])\n",
      "Label batch dimensions torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "286178e5-6c9f-4e90-a19c-754edcbf75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca209c52-413a-4f1f-8f91-c59ee84d897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False}        # Query-Key-Value bias\n",
    "\n",
    "cfg = GPT_CONFIG_124M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "280b5ff3-54ef-4e9e-b908-ba83ff98fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(cfg['emb_dim'], 4 * cfg['emb_dim'], bias = cfg['qkv_bias']), nn.GELU(), nn.Linear(4 * cfg['emb_dim'], cfg['emb_dim'], bias = cfg['qkv_bias']))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdc2e739-a3b3-4145-8844-024d890dce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(cfg['emb_dim']))\n",
    "        self.beta = nn.Parameter(torch.zeros(cfg['emb_dim']))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        x_norm_corrected = (self.gamma * x_norm) + self.beta\n",
    "        return x_norm_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9bfd178-a0dc-4446-8918-ff5279c869b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69e57b34-2e3f-4a60-a12e-1c29fb4e3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, cfg, d_out = 768, num_heads = cfg['n_heads'], context_length = cfg['context_length']):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        if d_out % self.num_heads != 0:\n",
    "            assert \"num_heads must be divisible by d_out\"    \n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(cfg['emb_dim'], self.d_out, bias = cfg['qkv_bias'])\n",
    "        self.W_key = nn.Linear(cfg['emb_dim'], self.d_out, bias = cfg['qkv_bias'])\n",
    "        self.W_value = nn.Linear(cfg['emb_dim'], self.d_out, bias = cfg['qkv_bias'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal = 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, emb_dim = x.shape\n",
    "        query = self.W_query(x)\n",
    "        key = self.W_key(x)\n",
    "        value = self.W_value(x)\n",
    "        query = query.view(b, num_tokens, self.num_heads, self.d_out // self.num_heads)\n",
    "        key = key.view(b, num_tokens, self.num_heads, self.d_out // self.num_heads)\n",
    "        value = value.view(b, num_tokens, self.num_heads, self.d_out // self.num_heads)\n",
    "\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        attn_scores = query @ key.transpose(2, 3)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1] ** 0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vectors = attn_weights @ value\n",
    "        context_vectors = context_vectors.transpose(1, 2).contiguous()\n",
    "        context_vectors = context_vectors.view(b, num_tokens, self.d_out)\n",
    "        context_vectors = self.out_proj(context_vectors) \n",
    "        return context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d355054-a650-47c0-aafc-88c228471e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = LayerNorm(cfg)\n",
    "        self.multiheadattention = MultiHeadAttention(cfg, num_heads = cfg['n_heads'])\n",
    "        self.dropout = Dropout(cfg)\n",
    "        self.layer_norm2 = LayerNorm(cfg)\n",
    "        self.feed_forward = FeedForward(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.multiheadattention(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa9e2f66-733c-49f9-b224-aebc4f8e58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        #self.encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.tok_embedding = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.dropout = Dropout(cfg)\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_layernorm = LayerNorm(cfg)\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias = cfg['qkv_bias'])\n",
    "        \n",
    "    def forward(self, tok_id):\n",
    "        batch, seq_len = tok_id.shape\n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len, device=tok_id.device))\n",
    "        tok_emb = self.tok_embedding(tok_id)\n",
    "\n",
    "        x = pos_emb + tok_emb\n",
    "        x = self.dropout(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_layernorm(x)\n",
    "        x = self.out_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0a392c9-aadd-4626-b55f-64c1cb966c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_encoding(text, tokenizer):\n",
    "    encoding = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7507b79e-312d-40e8-a949-b5a817b7b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(ids, tokenizer):\n",
    "    return tokenizer.decode(ids.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b91e189-5442-4520-9170-3a3ff2403e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x28b9772b670>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdba7cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.21 requires protobuf<5,>=4.25.3, but you have protobuf 6.33.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow >=2.15.0 tqdm >=4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f62c98b-5521-4843-bf15-61ec938bae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 15.4kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [01:32<00:00, 11.3kiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<?, ?iB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [53:49<00:00, 154kiB/s]     \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 332kiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:01<00:00, 301kiB/s]  \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 349kiB/s]  \n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7795510-c025-4fcf-8f16-6d06dd789f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right:{right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "391d4c59-ab49-447a-979a-435dbcadf76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    # 1. Position Embeddings\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    \n",
    "    # 2. Token Embeddings\n",
    "    gpt.tok_embedding.weight = assign(gpt.tok_embedding.weight, params['wte'])\n",
    "    \n",
    "    # 3. Iterate over Transformer Blocks\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        \n",
    "        # --- ATTENTION MECHANISM ---\n",
    "        # Get the Q, K, V weights from the fused 'c_attn' matrix\n",
    "        q_w, k_w, v_w = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        \n",
    "        # Load Weights (Transposed for PyTorch)\n",
    "        # Note: Your class uses .multiheadattention, not .att or .W_query directly on the block\n",
    "        gpt.trf_blocks[b].multiheadattention.W_query.weight = assign(gpt.trf_blocks[b].multiheadattention.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].multiheadattention.W_key.weight   = assign(gpt.trf_blocks[b].multiheadattention.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].multiheadattention.W_value.weight = assign(gpt.trf_blocks[b].multiheadattention.W_value.weight, v_w.T)\n",
    "        \n",
    "        # Load Biases\n",
    "        q_b, k_b, v_b = np.split((params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].multiheadattention.W_query.bias = assign(gpt.trf_blocks[b].multiheadattention.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].multiheadattention.W_key.bias   = assign(gpt.trf_blocks[b].multiheadattention.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].multiheadattention.W_value.bias = assign(gpt.trf_blocks[b].multiheadattention.W_value.bias, v_b)\n",
    "        \n",
    "        # --- ATTENTION OUTPUT PROJECTION (WARNING) ---\n",
    "        # Your 'MultiHeadAttention' class defined in Step 1 DOES NOT have an 'out_proj' layer.\n",
    "        # Standard GPT-2 weights have this. I have commented it out to prevent a crash.\n",
    "        # UNCOMMENT THE LINES BELOW IF YOU ADD 'self.out_proj' TO YOUR CLASS:\n",
    "        \n",
    "        gpt.trf_blocks[b].multiheadattention.out_proj.weight = assign(gpt.trf_blocks[b].multiheadattention.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].multiheadattention.out_proj.bias   = assign(gpt.trf_blocks[b].multiheadattention.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # --- FEED FORWARD NETWORK ---\n",
    "        # Layer 0: Expansion (c_fc)\n",
    "        gpt.trf_blocks[b].feed_forward.layers[0].weight = assign(gpt.trf_blocks[b].feed_forward.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].feed_forward.layers[0].bias   = assign(gpt.trf_blocks[b].feed_forward.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        \n",
    "        # Layer 2: Projection (c_proj) -> Note: Index 2 because Index 1 is GELU\n",
    "        gpt.trf_blocks[b].feed_forward.layers[2].weight = assign(gpt.trf_blocks[b].feed_forward.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].feed_forward.layers[2].bias   = assign(gpt.trf_blocks[b].feed_forward.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # --- LAYER NORMS ---\n",
    "        # Your class uses .layer_norm1/2, not .norm1/2\n",
    "        # Your class uses .gamma/.beta, not .scale/.shift\n",
    "        \n",
    "        # Norm 1 (Pre-Attention)\n",
    "        gpt.trf_blocks[b].layer_norm1.gamma = assign(gpt.trf_blocks[b].layer_norm1.gamma, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].layer_norm1.beta  = assign(gpt.trf_blocks[b].layer_norm1.beta, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        \n",
    "        # Norm 2 (Pre-FFN)\n",
    "        gpt.trf_blocks[b].layer_norm2.gamma = assign(gpt.trf_blocks[b].layer_norm2.gamma, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].layer_norm2.beta  = assign(gpt.trf_blocks[b].layer_norm2.beta, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    # 4. Final Layer Norm\n",
    "    gpt.final_layernorm.gamma = assign(gpt.final_layernorm.gamma, params[\"g\"])\n",
    "    gpt.final_layernorm.beta  = assign(gpt.final_layernorm.beta, params[\"b\"])\n",
    "\n",
    "    # 5. Output Head (Weight Tying)\n",
    "    # We reuse the token embedding weights for the output head\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cec47fae-1ef9-46ce-a156-a3f46186eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f0010-c32a-443a-a832-cb3126269bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (tok_embedding): Embedding(50257, 768)\n",
       "  (dropout): Dropout(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (layer_norm1): LayerNorm()\n",
       "      (multiheadattention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layernorm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()\n",
    "\n",
    "#load_weights_into_gpt(gpt, params)\n",
    "#gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "613b509a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162963456"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = GPTModel(cfg)\n",
    "gpt.eval()\n",
    "sum_ = 0\n",
    "for p in gpt.parameters():\n",
    "    sum_ = sum_ + p.numel()\n",
    "\n",
    "sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "968098ac-61b4-4b4b-8c74-e966cdfe3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k:\n",
    "            top_logits, top_pos = torch.topk(logits, top_k)\n",
    "            logits = torch.where(\n",
    "                condition=logits < top_logits[0][-1],   \n",
    "                input=torch.tensor(float('-inf')).to(logits.device),    \n",
    "                other=logits \n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim = -1)\n",
    "            next_token = torch.multinomial(probs, num_samples = 1)\n",
    "        else:\n",
    "            probs = torch.softmax(logits, dim = -1)\n",
    "            next_token = torch.argmax(probs, keepdim = True)\n",
    "\n",
    "        idx = torch.cat((idx, next_token), dim = 1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c26b4a9b-eb40-4049-85a1-dae33cc95851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time the United States began to use chemical weapons (and that was not before the war) the regime started to use the word \"terrorist\" in a similar way as it has used violence and propaganda to describe terrorist organizations. One way it has described terrorists is the fact that the Syrian and Iraqi governments are not at war, there are terrorist organizations all over the world and they're all doing war with a different type of country that has nothing but a high level of militarily significant resources like Israel. We've been told the government can not use chemical weapons on any of its opponents, but those who use it are not allowed within the scope of U.S. sovereignty or to use it. So the Syrians did this, then those other countries had a choice. This is what they did on 9-18-2011 and when the war is going on, if they don't have the U.S. support, they'll say that they want us to take back control by force. It doesn't matter what other groups you're talking about, they will have a weapon and they're doing war against them for a lot longer. It is still our right to try, because any organization or individuals that wants to use it will, if they know you, try, just wait, but they're trying to get some weapons that the Syrians want to use so it doesn't matter whether or not we attack them, they have weapons. And, if you get caught with what they've done, you could lose a lot of your money because the Syrians would be very frustrated because, no matter how much we go along, they are going to attack and the U: the government will be a part of that attack as long as we continue to try and bring people to their senses and get rid of them.\"\n",
      "\n",
      "\"If you can see that this is coming to fruition, we're ready for it now if they're willing to talk to us and we don't think we need an intervention in Syria now to try and bring it under control now. So I would encourage all those interested in this to read the comments there and join us to discuss it, but I'm not really willing to give an open-ended opinion. I'm not going to give any final order because when we've got to go out of this mess to take care of other groups like al Shabaab and ISIS we need to work on those fronts. And then the people who are in control of ISIS and Assad have been making statements\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=torch.tensor(tokenizer.encode(\"Once upon a time\")).to(device).unsqueeze(0),\n",
    "    max_new_tokens=500,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids.detach().clone().to('cpu').numpy(), tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef9bd76e-ec45-44dc-833d-e93559a61af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in gpt.parameters():\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe8d6f1-8d77-4293-9809-babf740e393a",
   "metadata": {},
   "source": [
    "# Change the classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3cc9ca36-901e-49b0-bd89-56de112fe0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.out_head = nn.Linear(cfg['emb_dim'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c4188b2-fe7f-475b-8da7-e49b7c40fb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.out_head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1efc1be-8e30-4529-9552-3080c4e252b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.out_head.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "709a238c-d1ac-41fc-89d1-9317f6157d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in gpt.trf_blocks[-1].parameters():\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d8bca31-7d77-4650-a5c1-1572c2ccb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in gpt.final_layernorm.parameters():\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "096ababf-7976-41c3-b628-e7894ba6a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc27f55a-c252-44ca-a604-51a4d1f4ed0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5840,  0.9893],\n",
      "         [-3.7231,  7.4521],\n",
      "         [-2.2665,  6.6035],\n",
      "         [-3.5974,  3.9888]]], device='cuda:0')\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = gpt(inputs.to(device))\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48c160af-8384-4ff4-b0e9-55342d913375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5974,  3.9888]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b9468f5-8584-4cad-8942-4a5912a2cc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b9901f1-8ea8-48a8-86f9-d6495eb85b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(dataloader, model, device, num_batches = None):\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    correct_labels = 0\n",
    "    for index, (inputs, label) in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs.to(device))\n",
    "        #print(output.shape)\n",
    "        output = output[:, -1, :]\n",
    "        #print(output.shape)\n",
    "        #print(torch.argmax(torch.softmax(output, dim = -1), dim = -1))\n",
    "        #print(label)\n",
    "        prediction = torch.argmax(torch.softmax(output, dim = -1), dim = -1)\n",
    "        accuracy += (prediction == label.to(device)).sum()\n",
    "        correct_labels += len(label)\n",
    "    score = (accuracy / correct_labels).item()\n",
    "    model.train()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efb90be0-5e55-4780-a50e-192bdc522ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49473685026168823"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy_loader(train_loader, gpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29da601c-3a90-4162-b3a1-06c318ae0a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 49.47%\n",
      "Validation accuracy: 53.02%\n",
      "Test accuracy: 50.33%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, gpt, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, gpt, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, gpt, device, num_batches=10\n",
    ")\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e59f859-3596-4c96-9deb-cebaf1bb5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    outputs = gpt(input_batch.to(device))\n",
    "    outputs = outputs[:, -1, :]\n",
    "    prediction = torch.softmax(outputs, dim = -1)\n",
    "    return nn.functional.cross_entropy(prediction, target_batch.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ef062e5-05e3-4e66-9aaa-dd335fe85aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(dataloader, model, device, num_batches = None):\n",
    "    total_loss = 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for index, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if index < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)       \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2eef718-9dcd-4d96-863d-1f82b776e422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7984588608031965"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_loss_loader(train_loader, gpt, device, num_batches = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d343fd6a-705e-4226-a6d2-fe1896ef55e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.686\n",
      "Validation loss: 0.806\n",
      "Test loss: 0.750\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():                \n",
    "    train_loss = calc_loss_loader(train_loader, gpt, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, gpt, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, gpt, device, num_batches=5)\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af0239e4-84d6-4751-b596-e82a3cb4cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(model, dataloader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []  \n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for index, (input_batch, target_batch) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            if global_step % eval_freq == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                \n",
    "                    train_loss = calc_loss_loader(train_loader, gpt, device, num_batches=eval_iter)\n",
    "                    val_loss = calc_loss_loader(val_loader, gpt, device, num_batches=eval_iter)\n",
    "                model.train()\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        train_acc = calc_accuracy_loader(train_loader, model, device, num_batches = eval_iter)\n",
    "        val_acc = calc_accuracy_loader(val_loader, model, device, num_batches = eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_accs)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b695433f-3cb3-4e05-92d1-4671296c3453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.687, Val loss 10.727\n",
      "Ep 1 (Step 000050): Train loss 10.450, Val loss 10.300\n",
      "Ep 1 (Step 000100): Train loss 10.125, Val loss 10.325\n",
      "Training accuracy: 56.46% | Validation accuracy: 58.39%\n",
      "Ep 2 (Step 000150): Train loss 10.250, Val loss 10.275\n",
      "Ep 2 (Step 000200): Train loss 10.350, Val loss 10.400\n",
      "Ep 2 (Step 000250): Train loss 10.325, Val loss 10.350\n",
      "Training accuracy: 56.46% | Validation accuracy: 58.39%\n",
      "Ep 3 (Step 000300): Train loss 10.275, Val loss 10.225\n",
      "Ep 3 (Step 000350): Train loss 10.400, Val loss 10.325\n",
      "Training accuracy: 56.46% | Validation accuracy: 58.39%\n",
      "Ep 4 (Step 000400): Train loss 10.350, Val loss 10.400\n",
      "Ep 4 (Step 000450): Train loss 10.275, Val loss 10.300\n",
      "Ep 4 (Step 000500): Train loss 10.350, Val loss 10.300\n",
      "Training accuracy: 56.46% | Validation accuracy: 58.39%\n",
      "Ep 5 (Step 000550): Train loss 10.350, Val loss 10.350\n",
      "Ep 5 (Step 000600): Train loss 10.400, Val loss 10.275\n",
      "Ep 5 (Step 000650): Train loss 10.400, Val loss 10.300\n",
      "Training accuracy: 56.46% | Validation accuracy: 58.39%\n",
      "Training completed in 67.39 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "    train_classifier_simple(gpt, train_loader, val_loader, optimizer, device, num_epochs = num_epochs, eval_freq = 50, eval_iter = 5)\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6944884-fb8f-42f8-9ae5-fe7cc0b8e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 49.47%\n",
      "Validation accuracy: 53.02%\n",
      "Test accuracy: 50.33%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, gpt, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, gpt, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, gpt, device)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf33a8-ba66-47cc-ba40-84e2c7097dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "createdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
